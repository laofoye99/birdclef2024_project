{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# libraries for setting up the environment\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "os.path.join('./')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from typing import Tuple\n",
    "# libraries for scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = mpl.colormaps.get_cmap('coolwarm')\n",
    "# libraries for audio processing\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "# from scipy import signal\n",
    "from IPython.display import Audio\n",
    "# libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import keras_cv\n",
    "# import keras\n",
    "# import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "# from glob import glob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# Load the audio processing functions from the provided files\n",
    "from audio import process, extraction, reduction, augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available: []\n",
      "Tensorflow version: 2.15.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "# print(\"Keras version:\", keras.__version__)\n",
    "# print(\"Keras CV version:\", keras_cv.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configure:\n",
    "    data_path = '/home/max/Desktop/birdclef/data'\n",
    "    audio_path = '/home/max/Desktop/birdclef/data/train_audio'\n",
    "    file_name = 'train_metadata.csv'\n",
    "    \n",
    "    seed = 42\n",
    "    \n",
    "    img_size = [128, 384]\n",
    "    batch_size = 16\n",
    "    \n",
    "    duration = 5\n",
    "    sample_rate = 32000\n",
    "    audio_length = duration * sample_rate\n",
    "    \n",
    "    nfft = 1024\n",
    "    frame_length = 1024\n",
    "    frame_step = 512\n",
    "    n_mels = 128\n",
    "    window = 1024\n",
    "    hop_length = 512\n",
    "    fmin = 20\n",
    "    fmax = 12000\n",
    "    \n",
    "    epochs = 10\n",
    "    preset = 'efficientnetv2_b2_imagenet'\n",
    "    \n",
    "    augment = True\n",
    "    \n",
    "    class_names = sorted(os.listdir(audio_path))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v: k for k, v in label2name.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(Configure.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_pipeline(file_path: str) -> Tuple [ np.ndarray, int, np.ndarray]:\n",
    "    # Load audio\n",
    "    audio, sample_rate = process.process_audio(file_path, Configure.sample_rate, Configure.audio_length)\n",
    "\n",
    "    # reduce noise\n",
    "    audio_clean = reduction.clean_audio(audio, sample_rate)\n",
    "    \n",
    "    # Extract features\n",
    "    magnitude_audio, phase_audio, mfccs, chroma, mel_spectrogram = extraction.extract_audio(audio_clean, sample_rate)\n",
    "    features = np.concatenate([magnitude_audio, phase_audio, mfccs, chroma, mel_spectrogram], axis=1)\n",
    "    \n",
    "    # Augment audio\n",
    "    augmented_audio = augmentation.augment_audio(audio_clean, sample_rate)\n",
    "\n",
    "    return augmented_audio, sample_rate, features\n",
    "\n",
    "def save_spectrogram(audio: np.ndarray, sr: int, output_path: str):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=12000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "def process_and_save_spectrograms(df: pd.DataFrame, output_dir: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    feature_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        processed_audio, sample_rate, features = process_audio_pipeline(file_path)\n",
    "\n",
    "        # Save spectrogram\n",
    "        output_path = os.path.join(output_dir, f\"processed_{row['file_name']}.png\")\n",
    "        save_spectrogram(processed_audio, sample_rate, output_path)\n",
    "        print(f\"Processed spectrogram saved to: {output_path}\")\n",
    "        \n",
    "        feature_list.append(features)\n",
    "    \n",
    "    feature_df = pd.DataFrame(feature_list, columns=['magnitude_audio', 'phase_audio', 'mfccs', 'chroma', 'mel_spectrogram'])\n",
    "    \n",
    "    result_df = pd.concat([df.reset_index(drop=True), feature_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the metadata\n",
    "metadata = pd.read_csv(f'{Configure.data_path}/train_metadata.csv')\n",
    "metadata['file_path'] = metadata.apply(lambda x: f\"{Configure.audio_path}/{x['filename']}\", axis=1)\n",
    "metadata['label'] = metadata['primary_label'].map(Configure.name2label)\n",
    "metadata['file_name'] = metadata['file_path'].map(lambda x: x.split('/')[-1].split('.')[0])\n",
    "metadata = metadata[['file_name', 'latitude', 'longitude', 'rating', 'label', 'file_path']]\n",
    "\n",
    "processed_metadata = process_and_save_spectrograms(metadata.iloc[:,:100], f'{Configure.data_path}/processed_spectrograms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(metadata, test_size=0.2)\n",
    "print(f\"Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO List:\n",
    "1. Load audio\n",
    "2. Data Exploration\n",
    "3. Resample audio\n",
    "4. Crop or Pad audio\n",
    "5. Standardize and Normalize audio\n",
    "6. Frame and Window audio\n",
    "7. Trim audio (select frequency range)\n",
    "8. Extract features (spectrogram, mel spectrogram, MFCC)\n",
    "9. Augment audio (time shift, pitch shift, speed tuning, noise injection, mixup, mixmatch, cutmix, specaugment, time masking, frequency masking)\n",
    "10. Split audio data into training and validation sets\n",
    "11. Build data pipeline\n",
    "12. Finetune model (efficientnetv2_b2_imagenet, or bird_vocalization_classifier) and Hyperparameter tuning\n",
    "13. Model evaluation\n",
    "\"\"\"\n",
    "\n",
    "def build_decoder(with_labels=True, dim=1024):\n",
    "    def get_audio(filepath):\n",
    "        audio = tfio.audio.AudioIOTensor(filepath)\n",
    "        sr = audio.rate.numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_bytes = tf.io.read_file('/home/max/Desktop/birdclef/data/train_audio/asbfly/XC49755.ogg')\n",
    "# print(file_bytes)\n",
    "\n",
    "# print(audio)\n",
    "\n",
    "\n",
    "audio = tfio.audio.AudioIOTensor('/home/max/Desktop/birdclef/data/train_audio/asbfly/XC49755.ogg')\n",
    "\n",
    "audio_slice = audio[:Configure.audio_length]\n",
    "\n",
    "audio_tensor = tf.squeeze(audio_slice, axis=[-1])\n",
    "\n",
    "display(Audio(audio_tensor.numpy(), rate=audio.rate.numpy()))\n",
    "display(plt.plot(audio_tensor.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = tfio.audio.trim(audio_tensor, axis=0, epsilon=0.1, name='trim_silence')\n",
    "print(position)\n",
    "\n",
    "start = position[0]\n",
    "stop = position[1]\n",
    "print(start, stop)\n",
    "\n",
    "processed = audio_tensor[start:stop]\n",
    "display(Audio(processed.numpy(), rate=audio.rate.numpy()))\n",
    "print(audio.rate.numpy())\n",
    "\n",
    "display(plt.plot(processed.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fade = tfio.audio.fade(processed, fade_in=1000, fade_out=2000, mode='logarithmic')\n",
    "plt.plot(fade.numpy())\n",
    "display(Audio(fade.numpy(), rate=audio.rate.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = tfio.audio.spectrogram(fade, nfft=Configure.nfft, window=Configure.window, stride=Configure.hop_length)\n",
    "plt.imshow(tf.math.log(spectrogram).numpy(),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = tfio.audio.melscale(spectrogram, rate=Configure.sample_rate, mels=Configure.n_mels, fmin=Configure.fmin, fmax=Configure.fmax)\n",
    "plt.imshow(tf.math.log(mel_spectrogram).numpy(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbScale_mel_spectrogram = tfio.audio.dbscale(mel_spectrogram, top_db=80)\n",
    "plt.imshow(dbScale_mel_spectrogram.numpy(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_mask = tfio.audio.freq_mask(dbScale_mel_spectrogram, param=50)\n",
    "plt.imshow(freq_mask.numpy(),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mask = tfio.audio.time_mask(dbScale_mel_spectrogram, param=40)\n",
    "plt.imshow(time_mask.numpy(),cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, duration):\n",
    "    audio = tfio.audio.AudioIOTensor(file_path)\n",
    "    audio_slice = audio[:duration]\n",
    "    audio_tensor = tf.squeeze(audio_slice, axis=[-1])\n",
    "    return audio_tensor.numpy(), audio.rate.numpy()\n",
    "\n",
    "def resample_audio(data, sample_rate, target_sample_rate):\n",
    "    if sample_rate != target_sample_rate:\n",
    "        audio= tfio.audio.resample(data, sample_rate, target_sample_rate, name='resampled_audio')\n",
    "        return audio.numpy(), target_sample_rate\n",
    "    else:\n",
    "        return data, sample_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = load_audio('/home/max/Desktop/birdclef/data/train_audio/asbfly/XC164848.ogg', Configure.audio_length)\n",
    "print(audio, sr)\n",
    "audio, sr = resample_audio(audio, sr, Configure.sample_rate)\n",
    "print(audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, target_sample_rate=16000, audio_length=16000):\n",
    "    # Load audio file\n",
    "    audio = tfio.audio.AudioIOTensor(file_path)\n",
    "    # Squeeze the audio tensor to remove the channel dimension\n",
    "    audio_squeeze = tf.squeeze(audio.to_tensor(), axis=[-1])\n",
    "    \n",
    "    # Resample audio if necessary\n",
    "    if audio.rate.numpy() != target_sample_rate:\n",
    "        audio_resample = tfio.audio.resample(audio_squeeze, rate_in=audio.rate.numpy(), \n",
    "                                            rate_out=target_sample_rate, name='resampled_audio')\n",
    "    else:\n",
    "        audio_resample = audio_squeeze\n",
    "    \n",
    "    # Crop or pad audio to the target length\n",
    "    length = tf.shape(audio_resample)[0]\n",
    "    if length > audio_length:\n",
    "        audio_crop = audio_resample[:audio_length]\n",
    "    elif length < audio_length:\n",
    "        paddings = [[0, audio_length - length]]\n",
    "        audio_crop = tf.pad(audio_resample, paddings, \"CONSTANT\")\n",
    "    else:\n",
    "        audio_crop = audio_resample\n",
    "    \n",
    "    # Standardize audio\n",
    "    mean = tf.math.reduce_mean(audio_crop)\n",
    "    std = tf.math.reduce_std(audio_crop)\n",
    "    audio_standardize = tf.where(tf.math.equal(std, 0), audio_crop - mean, (audio_crop - mean) / std)\n",
    "    \n",
    "    # Normalize using Min-Max scaling\n",
    "    min_val = tf.math.reduce_min(audio_standardize)\n",
    "    max_val = tf.math.reduce_max(audio_standardize)\n",
    "    audio_normalize = tf.where(tf.math.equal(max_val - min_val, 0), \n",
    "                                audio_standardize - min_val, (audio_standardize - min_val) / (max_val - min_val))\n",
    "    \n",
    "    return audio_normalize, target_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = load_audio('/home/max/Desktop/birdclef/data/train_audio/asbfly/XC49755.ogg', Configure.sample_rate, Configure.audio_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvForBird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
