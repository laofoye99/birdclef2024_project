{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 23:29:36.323580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-10 23:29:36.323828: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-10 23:29:36.364820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.colormaps.get_cmap('coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/max/Desktop/birdclef/data/train_audio/asbfly/XC842684.ogg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spectrogram(audio: np.ndarray, sr: int, output_path: str):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=12000)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x71433b5065c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.4942573 0.0062996363 -24.090977 0.29806325 0.36198068\n"
     ]
    }
   ],
   "source": [
    "from audio import process, reduction, augmentation, extraction\n",
    "Configure = {'target_sample_rate': 44100, 'target_duration': 10, }\n",
    "audio, sample_rate = process.process_audio(data_path,Configure['target_sample_rate'], Configure['target_duration'])\n",
    "audio_clean = reduction.clean_audio(audio, sample_rate)\n",
    "magnitude_audio, phase_audio, mfccs, chroma, mel_spectrogram = extraction.extract_audio(audio_clean, sample_rate)\n",
    "augmented_audio = augmentation.augment_audio(audio_clean, sample_rate)\n",
    "# display(Audio(augmented_audio, rate=sample_rate))\n",
    "# save_spectrogram(augmented_audio, sample_rate, 'audio_clean.png')\n",
    "print(magnitude_audio, phase_audio, mfccs, chroma, mel_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24459, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metadata = pd.read_csv('/home/max/Desktop/birdclef/data/train_metadata.csv')\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "audio, sample_rate = librosa.load(data_path, sr=None)\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveshow(audio, sr=sample_rate)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveshow(augmented_audio, sr=sample_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "# # 10. Split audio data into training and validation sets\n",
    "# def split_data(data, labels, test_size):\n",
    "#     return train_test_split(data, labels, test_size=test_size)\n",
    "#\n",
    "# # 11. Build data pipeline\n",
    "# def build_data_pipeline(data, labels):\n",
    "#     # Implementation depends on the library you're using\n",
    "#     pass\n",
    "#\n",
    "# # 12. Finetune model and Hyperparameter tuning\n",
    "# def finetune_model():\n",
    "#     # Implementation depends on the model you're using\n",
    "#     pass\n",
    "#\n",
    "# # 13. Model evaluation\n",
    "# def evaluate_model(model, test_data, test_labels):\n",
    "#     # Implementation depends on the model you're using\n",
    "#     pass\n",
    "#\n",
    "# # Main function to orchestrate the steps\n",
    "# def main():\n",
    "#     # Load data\n",
    "#     data = load_audio('path_to_your_audio_file')\n",
    "#\n",
    "#     # Explore data\n",
    "#     explore_data(data)\n",
    "#\n",
    "#     # Preprocess and augment data\n",
    "#     # ...\n",
    "#\n",
    "#     # Split data\n",
    "#     train_data, test_data, train_labels, test_labels = split_data(data, labels, test_size=0.2)\n",
    "#\n",
    "#     # Build data pipeline\n",
    "#     # ...\n",
    "#\n",
    "#     # Finetune model\n",
    "#     model = finetune_model()\n",
    "#\n",
    "#     # Evaluate model\n",
    "#     evaluate_model(model, test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvForBird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
